{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTTVdMssxEgE"
      },
      "source": [
        "# ***OCON Model Analysis***\n",
        "*(3-features all_speakers Dataset)*\n",
        "\n",
        "**Author:** S. Giacomelli\n",
        "\n",
        "**Year:** 2023\n",
        "\n",
        "**Affiliation:** A.Casella Conservatory (student)\n",
        "\n",
        "**Master Degree Thesis**: \"*Vowel phonemes Analysis & Classification by means of OCON rectifiers Deep Learning Architectures*\"\n",
        "\n",
        "**Description:** Python scripts for One-Class-One-Network (OCON) Model analysis and optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0yQDCRgxCbn"
      },
      "outputs": [],
      "source": [
        "# Numerical computations packages/modules\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Dataset processing modules\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Graphic visualization modules\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib_inline\n",
        "matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n",
        "\n",
        "# Common Seed initialization\n",
        "SEED = 42  # ... the answer to the ultimate question of Life, the Universe, and Everything... (cit.)\n",
        "\n",
        "# PyTorch Processing Units evaluation\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'AVILABLE Processing Unit: {device.upper()}')\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzyJTDWAVzN0"
      },
      "source": [
        "## **HGCW Dataset**\n",
        "\n",
        "- *Dataset_utils.npz* file read\n",
        "- *One-Hot encoding* definition\n",
        "- *Train/Dev/Test split* definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKqWKW4AVyW6"
      },
      "outputs": [],
      "source": [
        "# Load HGCW 4_features_all Dataset\n",
        "HGCW_dataset_utils = np.load(file='./HGCW_dataset_utils.npz')\n",
        "print('Raw features                    Data shape:', HGCW_dataset_utils['HGCW_raw'].shape)\n",
        "print('Fundamental Normalized features Data shape:', HGCW_dataset_utils['HGCW_fund_norm'].shape)\n",
        "print('MinMax features                 Data shape:', HGCW_dataset_utils['HGCW_minmax'].shape)\n",
        "print('Labels                          Data shape:', HGCW_dataset_utils['HGCW_labels'].shape)\n",
        "print('Classes size                    Data shape:', HGCW_dataset_utils['classes_size'].shape)\n",
        "print('Classes indices                 Data shape:', HGCW_dataset_utils['classes_idx'].shape)\n",
        "\n",
        "x_data_raw_np = HGCW_dataset_utils['HGCW_raw']\n",
        "x_data_fund_norm = HGCW_dataset_utils['HGCW_fund_norm']\n",
        "x_data_minmax = HGCW_dataset_utils['HGCW_minmax']\n",
        "y_labels_raw_np = HGCW_dataset_utils['HGCW_labels']\n",
        "vow_size = HGCW_dataset_utils['classes_size']\n",
        "end_idx = HGCW_dataset_utils['classes_idx']\n",
        "\n",
        "# Auxiliary lists\n",
        "vowels = ['ae', 'ah', 'aw', 'eh', 'er', 'ei', 'ih', 'iy', 'oa', 'oo', 'uh', 'uw']  # Vowels list\n",
        "colors = ['red', 'saddlebrown', 'darkorange', 'darkgoldenrod', 'gold', 'darkkhaki', 'olive', 'darkgreen', 'steelblue', 'fuchsia', 'indigo', 'black']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6HoOvoZWd9M"
      },
      "outputs": [],
      "source": [
        "# Class-specific One-hot encoding (Binarization)\n",
        "def one_hot_encoder(sel_class_number: int = 3, dataset: np.ndarray = x_data_minmax, orig_labels: int = len(vowels), classes_size: list = vow_size, classes_idx: list = end_idx, debug=False):\n",
        "\n",
        "    classes = [n for n in range(orig_labels)]  # Class Labels list initialization\n",
        "\n",
        "    # Auxiliary Parameters Initialization\n",
        "    if sel_class_number < len(classes):\n",
        "        classes.remove(sel_class_number)  # REST Classes list\n",
        "        if debug is True:\n",
        "            print(f'Selected Class \"{vowels[sel_class_number]}\" : {classes_size[sel_class_number]} samples')\n",
        "        sub_classes_size = classes_size[sel_class_number] // len(classes)\n",
        "        if debug is True:\n",
        "            print(f'Rest Classes size (...each): {sub_classes_size} samples')\n",
        "\n",
        "        # 1-Subset processing\n",
        "        sub_data = dataset[classes_idx[sel_class_number]: classes_idx[sel_class_number + 1], :]  # Selected Class feature slicing\n",
        "        sub_data_labels_bin = np.ones((classes_size[sel_class_number], 1), dtype='int')  # Selected Class labels (1) creation\n",
        "        sub_data_labels = np.ones((classes_size[sel_class_number], 1), dtype='int') * sel_class_number\n",
        "\n",
        "        # 0-Subset processing\n",
        "        for i in classes:\n",
        "            class_i_indices = np.random.choice(np.arange(classes_idx[i], classes_idx[i + 1], 1), size=sub_classes_size, replace=False)\n",
        "            sub_class_i_array = dataset[class_i_indices, :]\n",
        "            sub_class_labels_bin_array = np.zeros((sub_class_i_array.shape[0], 1), dtype='int')  # Rest I-esimal Class labels (0) creation\n",
        "            sub_class_labels_array = np.ones((sub_class_i_array.shape[0], 1), dtype='int') * i\n",
        "\n",
        "            # Outputs append\n",
        "            sub_data = np.vstack((sub_data, sub_class_i_array))\n",
        "            sub_data_labels_bin = np.vstack((sub_data_labels_bin, sub_class_labels_bin_array))\n",
        "            sub_data_labels = np.vstack((sub_data_labels, sub_class_labels_array))\n",
        "    else:\n",
        "        raise ValueError(f'Invalid Class ID: \"{sel_class_number}\" --> It must be less than {len(classes)}!')\n",
        "\n",
        "    return sub_data, sub_data_labels_bin, sub_data_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3tEYZ1TWm3T"
      },
      "outputs": [],
      "source": [
        "# Train/Test split (auxiliary function)\n",
        "def train_test_split_aux(features_dataset, labels_dataset, test_perc, tolerance):\n",
        "    \"\"\"\n",
        "    An auxiliary Train_Test_split function (based on Scikit Learn implementation) w. balance tolerance specification\n",
        "    \"\"\"\n",
        "    test_size = int(test_perc / 100 * len(features_dataset))\n",
        "    train_balance = 0  # Output Training set balance value initialization\n",
        "    test_balance = 0  # Output Testing set balance value initialization\n",
        "\n",
        "    min_tol = np.mean(labels_dataset) - tolerance\n",
        "    max_tol = np.mean(labels_dataset) + tolerance\n",
        "    print(f'Data Balancing  (TARGET = {np.mean(labels_dataset)} +- {tolerance}): ', end='')\n",
        "\n",
        "    while (min_tol >= train_balance or train_balance >= max_tol) or (min_tol >= test_balance or test_balance >= max_tol):\n",
        "        train_data, test_data, train_labels, test_labels = train_test_split(features_dataset, labels_dataset, test_size=test_size, shuffle=True)\n",
        "        train_balance = np.mean(train_labels)\n",
        "        test_balance = np.mean(test_labels)\n",
        "        print('.', end='')\n",
        "    else:\n",
        "        print('OK')\n",
        "\n",
        "    return train_data, test_data, train_labels, test_labels, train_balance, test_balance\n",
        "\n",
        "# Train-Dev-Test split function\n",
        "def train_dev_test_split(x_data, y_labels, split_list, tolerance=0.1, output='Loaders', debug=False):\n",
        "    \"\"\"\n",
        "    Compute a Train, Development (Hold-Out) and a Test set split w. PyTorch Dataset conversion (and eventual Loaders initialization)\n",
        "    \"\"\"\n",
        "    if len(split_list) == 3:\n",
        "        # Train - Dev+Test separation\n",
        "        print('Training --- Devel/Test SPLIT')\n",
        "        train_data, testTMP_data, train_labels, testTMP_labels, _, _ = train_test_split_aux(x_data, y_labels, (split_list[1] * 100) + (split_list[2] * 100), tolerance)\n",
        "        print('----------------------------------')\n",
        "\n",
        "        # Dev - Test separation\n",
        "        print('Devel    ---     Test SPLIT')\n",
        "\n",
        "        split = ((split_list[1] * 100) / np.sum(split_list[1:] * 100)) * 100  # Split in %\n",
        "        dev_data, test_data, dev_labels, test_labels, _, _ = train_test_split_aux(testTMP_data, testTMP_labels, split, tolerance)\n",
        "        print('----------------------------------')\n",
        "\n",
        "        # Tensor Conversion\n",
        "        train_data_tensor = torch.tensor(train_data).float()\n",
        "        train_labels_tensor = torch.tensor(train_labels, dtype=torch.int64).squeeze()\n",
        "        dev_data_tensor = torch.tensor(dev_data).float()\n",
        "        dev_labels_tensor = torch.tensor(dev_labels, dtype=torch.int64).squeeze()\n",
        "        test_data_tensor = torch.tensor(test_data).float()\n",
        "        test_labels_tensor = torch.tensor(test_labels, dtype=torch.int64).squeeze()\n",
        "        if debug is True:\n",
        "            print(f'Training Data        Shape: {train_data.shape}')\n",
        "            print(f'Development Data     Shape: {dev_data.shape}')\n",
        "            print(f'Testing Data         Shape: {test_data.shape}')\n",
        "\n",
        "            # Balance Evaluation\n",
        "            print(f'Training Set       Balance: {np.mean(train_labels)}')\n",
        "            print(f'Development Set    Balance: {np.mean(dev_labels)}')\n",
        "            print(f'Testing Set        Balance: {np.mean(test_labels)}')\n",
        "\n",
        "        if output != 'Loaders':\n",
        "            return train_data_tensor, train_labels_tensor, dev_data_tensor, dev_labels_tensor, test_data_tensor, test_labels_tensor\n",
        "        else:\n",
        "            # PyTorch Dataset Conversion\n",
        "            train_dataset = torch.utils.data.TensorDataset(torch.tensor(train_data).float(), torch.tensor(train_labels, dtype=torch.int64).squeeze())\n",
        "            dev_dataset = torch.utils.data.TensorDataset(torch.tensor(dev_data).float(), torch.tensor(dev_labels, dtype=torch.int64).squeeze())\n",
        "            test_dataset = torch.utils.data.TensorDataset(torch.tensor(test_data).float(), torch.tensor(test_labels, dtype=torch.int64).squeeze())\n",
        "\n",
        "            # DataLoader (Batches) --> Drop-Last control to optimize training\n",
        "            trainLoader = DataLoader(train_dataset, shuffle=False, batch_size = 32, drop_last=True)\n",
        "            devLoader = DataLoader(dev_dataset, shuffle=False, batch_size = dev_dataset.tensors[0].shape[0])\n",
        "            testLoader = DataLoader(test_dataset, shuffle=False, batch_size = test_dataset.tensors[0].shape[0])\n",
        "            if debug is True:\n",
        "                print(f'Training Set    Batch Size: {trainLoader.batch_size}')\n",
        "                print(f'Development Set Batch Size: {devLoader.batch_size}')\n",
        "                print(f'Testing Set     Batch Size: {testLoader.batch_size}')\n",
        "\n",
        "            return trainLoader, devLoader, testLoader\n",
        "    else:\n",
        "        # Train - Test separation\n",
        "        print('Training --- Test    SPLIT')\n",
        "        train_data, test_data, train_labels, test_labels, _, _ = train_test_split_aux(x_data, y_labels, split_list[1] * 100, tolerance, debug=debug)\n",
        "        print('--------------------------')\n",
        "\n",
        "        # Tensor Conversion\n",
        "        train_data_tensor = torch.tensor(train_data).float()\n",
        "        train_labels_tensor = torch.tensor(train_labels, dtype=torch.int64).squeeze()\n",
        "        test_data_tensor = torch.tensor(test_data).float()\n",
        "        test_labels_tensor = torch.tensor(test_labels, dtype=torch.int64).squeeze()\n",
        "        if debug is True:\n",
        "            print(f'Training Data        Shape: {train_data.shape}')\n",
        "            print(f'Testing Data         Shape: {test_data.shape}')\n",
        "\n",
        "            # Balance Evaluation\n",
        "            print(f'Training Set    Balance: {np.mean(train_labels)}')\n",
        "            print(f'Testing Set     Balance: {np.mean(test_labels)}')\n",
        "\n",
        "        if output != 'Loaders':\n",
        "            return train_data_tensor, train_labels_tensor, test_data_tensor, test_labels_tensor\n",
        "        else:\n",
        "            # PyTorch Dataset Conversion\n",
        "            train_dataset = torch.utils.data.TensorDataset(torch.tensor(train_data).float(), torch.tensor(train_labels, dtype=torch.int64).squeeze())\n",
        "            test_dataset = torch.utils.data.TensorDataset(torch.tensor(test_data).float(), torch.tensor(test_labels, dtype=torch.int64).squeeze())\n",
        "\n",
        "            # DataLoader (Batches) --> Drop-Last control to optimize training\n",
        "            trainLoader = DataLoader(train_dataset, shuffle=False, batch_size = 32, drop_last=True)\n",
        "            testLoader = DataLoader(test_dataset, shuffle=False, batch_size = test_dataset.tensors[0].shape[0])\n",
        "            if debug is True:\n",
        "                print(f'Training Set    Batch Size: {trainLoader.batch_size}')\n",
        "                print(f'Testing Set     Batch Size: {testLoader.batch_size}')\n",
        "\n",
        "            return trainLoader, testLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LD85TgYSod5U"
      },
      "source": [
        "## **One-Class Architecture** (Binary Classifier)\n",
        "(see \"*One-Class_Sub-Network_Analysis.ipynb*\")\n",
        "\n",
        "```\n",
        "Multi-Layer Perceptron\n",
        "- Input Layer: 3 features [formant ratios, min-max normalized]\n",
        "- Hidden Layer: 100 units\n",
        "- Output Layer: 1 normalized probability\n",
        "- Learning Rate: 0.0001 (10^-4)\n",
        "- Optimizer: Adam (Adaptive Momentum)\n",
        "\n",
        "- Mini-Batch Training:\n",
        "    . Re-iterated Sub-Dataset Shuffling\n",
        "    . Early Stopping (Test Accuracy driven)\n",
        "    . Batch size = 32\n",
        "\n",
        "- Regularization:\n",
        "    . Weight Decay (L2 Penalty): 0.0001 (10^-4)\n",
        "    . DropOut:\n",
        "        * Input Layer Drop Rate: 0.8\n",
        "        * Hidden Layer Drop Rate: 0.5.\n",
        "    . Batch Normalization\n",
        "```\n",
        "\n",
        "- *MLP Classifier Architecture* class definition\n",
        "- *Mini-Batch Training* function definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYOtpU5Ho0Ky"
      },
      "outputs": [],
      "source": [
        "# Dynamic Multi-Layer Architecture Class (w. units, activation function, batch normalization and dropOut rate specification)\n",
        "class binaryClassifier(nn.Module):                                                # nn.Module: base class to inherit from\n",
        "    def __init__(self, n_units, act_fun, rate_in, rate_hidden, model_name):                   # self + attributes (architecture hyper-parameters)\n",
        "        super().__init__()\n",
        "\n",
        "        self.layers = nn.ModuleDict()                                             # Dictionary to store Model layers\n",
        "        self.name = model_name\n",
        "\n",
        "        # Input Layer\n",
        "        self.layers['input'] = nn.Linear(3, n_units)                              # Key 'input' layer specification\n",
        "\n",
        "        # Hidden Layer\n",
        "        self.layers[f'hidden'] = nn.Linear(n_units, n_units)\n",
        "        self.layers[f'batch_norm'] = nn.BatchNorm1d(n_units)\n",
        "\n",
        "        # Output Layer\n",
        "        self.layers['output'] = nn.Linear(n_units, 1)                             # Key 'output' layer specification\n",
        "\n",
        "        # Activation Function\n",
        "        self.actfun = act_fun                                                     # Function string-name attribute association\n",
        "\n",
        "        # Dropout Parameter\n",
        "        self.dr_in = rate_in\n",
        "        self.dr_hidden = rate_hidden\n",
        "\n",
        "        # Weights & Bias initialization\n",
        "        for layer in self.layers.keys():\n",
        "            try:\n",
        "                nn.init.kaiming_normal_(self.layers[layer].weight, mode='fan_in') # Kaiming He - Normal Distributed (ReLU specific)\n",
        "            except:\n",
        "                pass                                                              # Batch_norm Layer can't be initialized\n",
        "            self.layers[layer].bias.data.fill_(0.)                                # Bias initialization (0.)\n",
        "\n",
        "    # Forward Pass Method\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Activation function object computation\n",
        "        actfun = getattr(torch.nn, self.actfun)\n",
        "\n",
        "        # Input Layer pass                                                        --> Weightening (Dot Product) \"Linear transform\" + \"Non linear function\" transform application\n",
        "        x = actfun()(self.layers['input'](x))\n",
        "        x = F.dropout(x, p=self.dr_in, training=self.training)                    # Activate DropOut only when Model Training == True\n",
        "\n",
        "        # Single Hidden Layer pass                                                --> Weightening (Dot Product) \"Linear transform\" + \"Non linear function\" transform application\n",
        "        x = self.layers[f'batch_norm'](x)                                         # Apply batch normalization before hidden layer\n",
        "        x = actfun()(self.layers[f'hidden'](x))\n",
        "        x = F.dropout(x, p=self.dr_hidden, training=self.training)                # Same as \"Input pass\"\n",
        "\n",
        "        # Output Layer pass                                                       --> Output Weightening (Dot Product) \"Linear transform\" (Optimizer implement an Output Sigmoid sctivation)\n",
        "        x = self.layers['output'](x)\n",
        "        x = nn.Sigmoid()(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7AZCONXJZuBv"
      },
      "outputs": [],
      "source": [
        "# Batch Training function (w. Adam Optimizer & L2 penalty term) Re-Definition\n",
        "def mini_batch_train_test(model, weight_decay, epochs: int, learning_rate, train_loader, dev_loader, test_loader, debug=False):\n",
        "    \"\"\"\n",
        "    Train & Test an ANN Architecture via Mini-Batch Training (w. Train/Dev/Test PyTorch Loaders) and Adam Backpropagation Optimizer\n",
        "    \"\"\"\n",
        "    # Loss Function initialization\n",
        "    loss_function = nn.BCELoss()\n",
        "\n",
        "    # Optimizer Algorithm initialization\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "    # Output list initialization\n",
        "    train_accuracies = []\n",
        "    train_losses = []\n",
        "    dev_accuracies = []\n",
        "\n",
        "    # TRAINING Phase\n",
        "    for epoch in range(epochs):\n",
        "        model.train()  # TRAINING Switch ON\n",
        "\n",
        "        batch_accuracies = []\n",
        "        batch_losses = []\n",
        "\n",
        "        # Training BATCHES Loop\n",
        "        for data_batch, labels_batch in train_loader:\n",
        "            train_predictions = model(data_batch)\n",
        "            train_loss = loss_function(train_predictions.squeeze(), labels_batch.type(torch.int64).float())\n",
        "            batch_losses.append(train_loss.detach())\n",
        "\n",
        "            # Backpropagation\n",
        "            optimizer.zero_grad()\n",
        "            train_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Accuracy\n",
        "            train_accuracy = 100 * torch.mean(((train_predictions.squeeze() > 0.5) == labels_batch.type(torch.int64).squeeze()).float())\n",
        "\n",
        "            # Batch Stats appending\n",
        "            batch_accuracies.append(train_accuracy.detach())\n",
        "            batch_losses.append(train_loss.detach())\n",
        "\n",
        "        # Training Stats appending\n",
        "        train_accuracies.append(np.mean(batch_accuracies))  # Average of Batch Accuracies = Training step accuracy\n",
        "        train_losses.append(np.mean(batch_losses))  # Average of Batch Losses = Training step Losses\n",
        "\n",
        "        # EVALUATION (Dev) Phase\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            dev_data_batch, dev_labels_batch = next(iter(dev_loader))\n",
        "            dev_predictions = model(dev_data_batch)\n",
        "\n",
        "            dev_accuracy = 100 * torch.mean(((dev_predictions.squeeze() > 0.5) == dev_labels_batch.type(torch.int64).squeeze()).float())\n",
        "\n",
        "            if debug is True:\n",
        "                if epoch % 100 == 0:\n",
        "                    print(f'Epoch {epoch} --> DEV ACCURACY: {dev_accuracy.detach():.3f} %')\n",
        "                    print('--------------------------------')\n",
        "\n",
        "            # Evaluation accuracy appending\n",
        "            dev_accuracies.append(dev_accuracy.detach())\n",
        "\n",
        "    # TEST Phase\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        test_data_batch, test_labels_batch = next(iter(test_loader))\n",
        "        test_predictions = model(test_data_batch)\n",
        "        test_accuracy = 100 * torch.mean(((test_predictions.squeeze() > 0.5) == test_labels_batch.type(torch.int64).squeeze()).float())\n",
        "\n",
        "        if debug is True:\n",
        "            print(f'TEST ACCURACY: {test_accuracy.detach():.2f} %')\n",
        "            print('--------------------------------------------------------------------')\n",
        "\n",
        "    return train_accuracies, train_losses, dev_accuracies, test_accuracy.detach()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JbqjG18pRnn"
      },
      "source": [
        "## **OCON (One-Class-One-Net)** Model\n",
        "\n",
        "Binary classifiers **Parallelization**\n",
        "-  *Classifiers-Bank* function definition\n",
        "    - *Models Parameters* inspection\n",
        "- Classifiers **Sequential** Training & Evaluation\n",
        "- *Models Parameters State Save/Load* function definition\n",
        "\n",
        "---\n",
        "\n",
        "- MaxNet output algorithm\n",
        "- Argmax output algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNdc0cIvsq0T"
      },
      "outputs": [],
      "source": [
        "def OCON_bank(one_class_function, hidden_units, act_fun, dr_in, dr_hidden, classes_list):\n",
        "    \"\"\"\n",
        "    Create a One-Class-One-Network parallelization bank of an input Sub-Network definition\n",
        "    \"\"\"\n",
        "    # Sub-Net names creation\n",
        "    models_name_list = []\n",
        "    for i in range(len(classes_list)):\n",
        "        models_name_list.append(\"{}_{}\".format(classes_list[i], \"subnet\"))  # Class name + _subnet\n",
        "\n",
        "    # Sub-Networks instances creation\n",
        "    sub_nets = []  # Sub Network list initialization\n",
        "\n",
        "    for i in range(len(models_name_list)):\n",
        "\n",
        "        torch.manual_seed(SEED)  # Seed re-initialization\n",
        "\n",
        "        # Sub-Net instance creation\n",
        "        locals()[models_name_list[i]] = one_class_function(hidden_units, act_fun, dr_in, dr_hidden, models_name_list[i])\n",
        "        sub_nets.append(locals()[models_name_list[i]])\n",
        "\n",
        "    return sub_nets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TgUnOZ0C-xLe"
      },
      "outputs": [],
      "source": [
        "# Load Architecture Parameters State function\n",
        "def load_model_state(model, state_dict_path):\n",
        "    \"\"\"\n",
        "    Load an existent State Dictionary in a defined model\n",
        "    \"\"\"\n",
        "\n",
        "    model.load_state_dict(torch.load(state_dict_path))\n",
        "    print(f'Loaded Parameters (from \"{state_dict_path}\") into: {model.name}')\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXjVg3GiAk3R"
      },
      "outputs": [],
      "source": [
        "# Build The OCON Model\n",
        "ocon_vowels = OCON_bank(binaryClassifier, 100, 'ReLU', 0.8, 0.5, vowels)  # Best MLP (see \"One-Class_Binary_Classifier_Analysis.ipynb\")\n",
        "\n",
        "# Load Pre-Trained Architectures in a fresh Model instance:\n",
        "#ocon_vowels = OCON_bank(binaryClassifier, 100, 'ReLU', 0.8, 0.5, vowels)  # Best MLP (see \"One-Class_Binary_Classifier_Analysis.ipynb\")\n",
        "#states_path = [\"Trained_models_state/ae_subnet_Params.pth\",\n",
        "#               \"Trained_models_state/ah_subnet_Params.pth\",\n",
        "#               \"Trained_models_state/aw_subnet_Params.pth\",\n",
        "#               \"Trained_models_state/eh_subnet_Params.pth\",\n",
        "#               \"Trained_models_state/er_subnet_Params.pth\",\n",
        "#               \"Trained_models_state/ei_subnet_Params.pth\",\n",
        "#               \"Trained_models_state/ih_subnet_Params.pth\",\n",
        "#               \"Trained_models_state/iy_subnet_Params.pth\",\n",
        "#               \"Trained_models_state/oa_subnet_Params.pth\",\n",
        "#               \"Trained_models_state/oo_subnet_Params.pth\",\n",
        "#               \"Trained_models_state/uh_subnet_Params.pth\",\n",
        "#               \"Trained_models_state/uw_subnet_Params.pth\"]\n",
        "#\n",
        "#for i in range(len(ocon_vowels)):\n",
        "#    load_model_state(ocon_vowels[i], states_path[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XmE7xerUAcEY"
      },
      "outputs": [],
      "source": [
        "# OCON Evaluation function\n",
        "def OCON_eval(ocon_models_bank, features_dataset: np.ndarray = x_data_minmax[:, 1:], labels: np.ndarray = y_labels_raw_np):\n",
        "    \"\"\"\n",
        "    Evaluate OCON models-bank over an entire dataset\n",
        "    \"\"\"\n",
        "    # Output lists initialization\n",
        "    predictions = []\n",
        "    dist_errors = []\n",
        "    eval_accuracies = []\n",
        "    g_truths = []  # For plotting purpouses\n",
        "\n",
        "    # Evaluate each Sub-Network...\n",
        "    for i in range(len(ocon_models_bank)):\n",
        "        ocon_models_bank[i].eval()  # Put j-esimal Sub-Network in Evaluation Mode\n",
        "        print(f'{ocon_models_bank[i].name.upper()} Evaluation -', end=' ')\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            # Make predictions\n",
        "            features_data_tensor = torch.tensor(features_dataset).float()\n",
        "            raw_eval_predictions = ocon_models_bank[i](features_data_tensor)\n",
        "\n",
        "            # Create Ground Truths\n",
        "            ground_truth = np.where(labels == i, 1, 0)\n",
        "            ground_truth_tensor = torch.tensor(ground_truth, dtype=torch.int64).squeeze()\n",
        "\n",
        "            # Compute Errors\n",
        "            dist_error = ground_truth_tensor - raw_eval_predictions.detach().squeeze()  # Distances\n",
        "            eval_accuracy = 100 * torch.mean(((raw_eval_predictions.detach().squeeze() > 0.5) == ground_truth_tensor).float())\n",
        "            print(f'Accuracy: {eval_accuracy:.2f}%')\n",
        "\n",
        "        # Outputs append\n",
        "        predictions.append(raw_eval_predictions.detach())\n",
        "        dist_errors.append(dist_error.detach())\n",
        "        eval_accuracies.append(eval_accuracy.detach())\n",
        "\n",
        "        g_truths.append(ground_truth)\n",
        "\n",
        "    return predictions, dist_errors, eval_accuracies, g_truths\n",
        "\n",
        "# For Pre-Trained Models\n",
        "#ocon_predictions, ocon_dist_errors, ocon_eval_accuracies, ocon_g_truths = OCON_eval(ocon_vowels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xKW4PuUWc4EB"
      },
      "outputs": [],
      "source": [
        "# Model Parameters State function\n",
        "def model_desc(model):\n",
        "    \"\"\"\n",
        "    Print a Console report of Neural Network Model parameters\n",
        "    \"\"\"\n",
        "    # Parameters Description\n",
        "    print('Params Description:')\n",
        "    trainable_params = 0\n",
        "\n",
        "    for parameter in model.named_parameters():\n",
        "        print(f'Parameter Name      : {parameter[0]}')\n",
        "        print(f'Parameter Weights   : {parameter[1][:]}')\n",
        "        if parameter[1].requires_grad:\n",
        "            print(f'...with {parameter[1].numel()} TRAINABLE parameters')\n",
        "            trainable_params += parameter[1].numel()\n",
        "\n",
        "        print('................................')\n",
        "\n",
        "    print('----------------------------------------------------------------')\n",
        "\n",
        "    # Nodes Count\n",
        "    nodes = 0\n",
        "    for param_name, param_tensor in model.named_parameters():\n",
        "        if 'bias' in param_name:\n",
        "            nodes += len(param_tensor)\n",
        "\n",
        "    print(f'Total Nodes             : {nodes}')\n",
        "    print('----------------------------------------------------------------')\n",
        "\n",
        "# OCON-Model Description\n",
        "for i in range(len(ocon_vowels)):\n",
        "    print(f'OCON \"{ocon_vowels[i].name}\" Classifier STATE')\n",
        "    model_desc(ocon_vowels[i])\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXr1ctCKlJAE"
      },
      "outputs": [],
      "source": [
        "# Training/Eval/Testing Parameters\n",
        "epochs = 1000  # For each \"Data Batch-Set\"\n",
        "loss_break = 0.20  # loss (for Early Stopping)\n",
        "acc_break = 90.  # % accuracy (for Early Stopping)\n",
        "min_tolerance = 0.01 # ...for sub-dataset balancing\n",
        "\n",
        "# Outputs Initialization\n",
        "loss_functions = [[] for _ in range(len(ocon_vowels))]\n",
        "training_accuracies = [[] for _ in range(len(ocon_vowels))]\n",
        "evaluation_accuracies = [[] for _ in range(len(ocon_vowels))]\n",
        "test_accuracies = [[] for _ in range(len(ocon_vowels))]\n",
        "training_times = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S_w_3q8P1L8i"
      },
      "outputs": [],
      "source": [
        "# OCON Sub-Networks Training\n",
        "from time import perf_counter\n",
        "debug = False\n",
        "\n",
        "for i, vowel in enumerate(vowels):\n",
        "    print(f'Architecture \"{ocon_vowels[i].name}\" TRAINING PHASE')\n",
        "\n",
        "    start_timer = perf_counter()\n",
        "    # Iterated (w. Batch-Sets shuffling) Mini-Batch Training\n",
        "    iteration = 0  # Batch Training iteration counter\n",
        "    mean_loss = 1.\n",
        "    test_accuracy = 0.\n",
        "\n",
        "    while (mean_loss > loss_break) or (test_accuracy < acc_break):\n",
        "        # Dataset processing\n",
        "        sub_data, sub_data_labels_bin, _ = one_hot_encoder(sel_class_number=i, dataset=x_data_minmax, debug=debug)\n",
        "        print('----------------------------------')\n",
        "        trainLoader, devLoader, testLoader = train_dev_test_split(sub_data[:, 1:], sub_data_labels_bin, [0.5, 0.25, 0.25], tolerance=min_tolerance, output='Loaders', debug=debug)\n",
        "\n",
        "        # Train/Test Architecture\n",
        "        train_accuracies, train_losses, dev_accuracies, test_accuracy = mini_batch_train_test(ocon_vowels[i], weight_decay=0.0001, epochs=epochs, learning_rate=0.0001, train_loader=trainLoader, dev_loader=devLoader, test_loader=testLoader, debug=debug)\n",
        "        print(f'Sub-Net \"{vowel.upper()}\" Epoch {(iteration + 1) * epochs} - TEST ACCURACY: {test_accuracy:.2f}%', end=' ')\n",
        "\n",
        "        # Outputs append\n",
        "        loss_functions[i].append(train_losses)\n",
        "        training_accuracies[i].append(train_accuracies)\n",
        "        evaluation_accuracies[i].append(dev_accuracies)\n",
        "        test_accuracies[i].append(test_accuracy)\n",
        "\n",
        "        # Repeating condition evaluation\n",
        "        mean_loss = np.mean(train_losses[-50: ])  # Last 100 losses mean\n",
        "        print(f'- MEAN LOSS: {mean_loss}')\n",
        "\n",
        "        iteration += 1  # Go to next Batch training iteration\n",
        "\n",
        "    print(f'Training STOPPED at iteration {iteration}')\n",
        "    print('--------------------------------------------------------------------')\n",
        "    stop_timer = perf_counter()\n",
        "\n",
        "    print(f'\"{ocon_vowels[i].name}\" Training COMPLETED in {float(stop_timer - start_timer)}sec.')\n",
        "    training_times.append(stop_timer - start_timer)\n",
        "    print('--------------------------------------------------------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TgJcnktn-bg-"
      },
      "outputs": [],
      "source": [
        "# Graphical smoothing filter\n",
        "def smooth(data, k=100):\n",
        "    \"\"\"\n",
        "    A Convolution LP filter w. interval definition\n",
        "    \"\"\"\n",
        "    return np.convolve(data, np.ones(k) / k, mode='same')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "szxOHsZI0tNE"
      },
      "outputs": [],
      "source": [
        "# Training Phase Plots\n",
        "plt.figure(figsize=(12, 5 * 12))\n",
        "\n",
        "# loss_functions, training_accuracies, evaluation_accuracies, test_accuracies, training_times\n",
        "classes = len(ocon_vowels)\n",
        "\n",
        "for i in range(classes):\n",
        "    plt.subplot(classes, 2, (i * 2) + 1)\n",
        "    flat_loss_function = [item for sublist in loss_functions[i] for item in sublist]\n",
        "\n",
        "    plt.plot(smooth(flat_loss_function), 'k-')\n",
        "    plt.axhline(loss_break, color='r', linestyle='--')\n",
        "    plt.title(f'{ocon_vowels[i].name.upper()} Training Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.xlim([100, len(flat_loss_function) - 100])\n",
        "    plt.ylabel('GT - Predicted diff. (probability)')\n",
        "    plt.grid()\n",
        "\n",
        "    plt.subplot(classes, 2, (i * 2) + 2)\n",
        "    flat_training_accuracy = [item for sublist in training_accuracies[i] for item in sublist]\n",
        "    flat_dev_accuracy = [item for sublist in evaluation_accuracies[i] for item in sublist]\n",
        "    flat_test_accuracy = test_accuracies[i]\n",
        "\n",
        "    plt.plot(smooth(flat_training_accuracy), 'k-', label='Training')\n",
        "    plt.plot(smooth(flat_dev_accuracy), color='grey', label='Development')\n",
        "    if len(flat_test_accuracy) > 1:\n",
        "        plt.plot([(n + 1) * epochs for n in range(len(flat_test_accuracy))], flat_test_accuracy, 'r-', label=f'Test')\n",
        "    else:\n",
        "        plt.axhline(test_accuracy, color='r', linestyle='-', label=f'Test')\n",
        "    plt.title(f'{ocon_vowels[i].name.upper()} Accuracy (after {training_times[i]:.2f}sec.)')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.xlim([100, len(flat_training_accuracy) - 100])\n",
        "    plt.ylabel('Accuracy (in %)')\n",
        "    plt.ylim([40, 101])\n",
        "    plt.grid()\n",
        "    plt.legend(loc='best')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('OCON_training_phase')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XaI3Z-_S21yd"
      },
      "outputs": [],
      "source": [
        "# OCON Evaluation\n",
        "ocon_predictions, ocon_dist_errors, ocon_eval_accuracies, ocon_g_truths = OCON_eval(ocon_vowels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_10MmVY9THbt"
      },
      "outputs": [],
      "source": [
        "# Dataset Evaluation Analysis Plot\n",
        "plt.figure(figsize=(22, 5 * len(ocon_vowels)))\n",
        "plot_ticks = end_idx[:]\n",
        "plot_ticks = np.delete(plot_ticks, -1)\n",
        "\n",
        "for i in range(len(ocon_vowels)):\n",
        "    plt.subplot(len(ocon_vowels), 3, (i * 3) + 1)\n",
        "    plt.plot(ocon_predictions[i], 'k.', label='Raw Predictions')\n",
        "    plt.plot(ocon_g_truths[i], 'rx', label='Ground Truths')\n",
        "    plt.axhline(0.5, linestyle='--', color='grey')\n",
        "    plt.title(f'{ocon_vowels[i].name.upper()} Predictions Accuracy: {ocon_eval_accuracies[i]:.2f}%')\n",
        "    plt.xlabel('Data (Indices)')\n",
        "    plt.xticks(ticks=plot_ticks, labels=vowels)\n",
        "    plt.ylabel('Normalized Probability')\n",
        "    plt.grid()\n",
        "    plt.legend(loc='best')\n",
        "\n",
        "    plt.subplot(len(ocon_vowels), 3, (i * 3) + 2)\n",
        "    plt.plot(ocon_dist_errors[i], 'k')\n",
        "    plt.title(f'Predicted to Measured Error')\n",
        "    plt.xlabel('Data (Indices)')\n",
        "    plt.xticks(ticks=plot_ticks, labels=vowels)\n",
        "    plt.ylabel('Normalized Probability Error')\n",
        "    plt.ylim([-1.1, 1.1])\n",
        "    plt.grid()\n",
        "\n",
        "    plt.subplot(len(ocon_vowels), 3, (i * 3) + 3)\n",
        "\n",
        "    # Predictions list processing\n",
        "    predictions_temp = ocon_predictions[i]\n",
        "    class_predictions = [item for sublist in predictions_temp for item in sublist]  # Turn a list of lists in a single list (of tensors)\n",
        "    for j in range(len(class_predictions)):  # Turn a list of tensors of one variable in a list of scalars (item() method)\n",
        "        class_predictions[j] = class_predictions[j].item()\n",
        "\n",
        "    # Positives & False-Positives extraction\n",
        "    positives = []\n",
        "    for w in range(len(vowels)):\n",
        "        num = (np.array(class_predictions[end_idx[w]: end_idx[w + 1]]) > 0.5).sum()\n",
        "        positives.append(num)\n",
        "\n",
        "    plt.bar(np.arange(len(vowels)), positives, color='k')\n",
        "    plt.title(f'\"{vowels[i]}\" Positive Probabilities Distribution')\n",
        "    plt.xlabel('Normalized Probabilities')\n",
        "    plt.ylabel('Occurences')\n",
        "    plt.xticks([n for n in range(12)], vowels)\n",
        "    plt.grid()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('OCON_bank_evaluation')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0uiLL--SR48_"
      },
      "outputs": [],
      "source": [
        "# Model Parameters Save/Load functions\n",
        "from pathlib import Path\n",
        "\n",
        "def save_model_state(model, folder_name: str = \"Trained_models_state\"):\n",
        "    \"\"\"\n",
        "    Save Pre-Trained model parameters in a State Dictionary\n",
        "    \"\"\"\n",
        "\n",
        "    MODEL_PATH = Path(folder_name)  # Placed in root\n",
        "    MODEL_PATH.mkdir(parents=True, exist_ok=True)  # Pre-existing folder (w. same name) monitoring\n",
        "    MODEL_NAME = '{}_{}'.format(model.name, \"Params.pth\")\n",
        "    MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
        "\n",
        "    print(f\"Saving {model.name} Parameters in: {MODEL_SAVE_PATH}\")\n",
        "    torch.save(obj=model.state_dict(), f=MODEL_SAVE_PATH)\n",
        "\n",
        "    return MODEL_SAVE_PATH\n",
        "\n",
        "# Save Pre-Trained Models-bank\n",
        "states_path = []  # Path for each model parameters state\n",
        "for i in range(len(ocon_vowels)):\n",
        "    state_path = save_model_state(ocon_vowels[i])\n",
        "    states_path.append(state_path)\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iykkdli0RrhE"
      },
      "source": [
        "### Output **Maxnet Algorithm**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LpKJjLewcMGY"
      },
      "outputs": [],
      "source": [
        "# OCON \"MaxNet\" Architecture (Weightening + Non Linearity apply)\n",
        "class OCON_MaxNet(nn.Module):                                             # nn.Module: base class to inherit from\n",
        "    def __init__(self, n_units, act_fun, eps):                                 # self + attributes (architecture hyper-parameters)\n",
        "        super().__init__()\n",
        "\n",
        "        self.layers = nn.ModuleDict()                                     # Dictionary to store Model layers\n",
        "        self.eps_weight = eps\n",
        "\n",
        "        # MaxNet Layer\n",
        "        self.layers['MAXNET'] = nn.Linear(n_units, n_units)               # Key 'MaxNet' layer specification\n",
        "\n",
        "        # Weights & Bias initialization\n",
        "        self.layers['MAXNET'].weight.data.fill_(self.eps_weight)\n",
        "        for i in range(n_units):\n",
        "            self.layers['MAXNET'].weight[i][i].data.fill_(1.)  # Self Weight = 1\n",
        "\n",
        "        self.layers['MAXNET'].bias.data.fill_(0.)\n",
        "\n",
        "        # Activation Function\n",
        "        self.actfun = act_fun  # Function string-name attribute association\n",
        "\n",
        "    # Forward Pass Method\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Activation function object computation\n",
        "        actfun = getattr(torch.nn, self.actfun)\n",
        "\n",
        "        # Maxnet Layer pass                                               --> Output Weightening (Dot Product) \"Linear transform\" + Non Linearity Activation Function\n",
        "        x = actfun()(self.layers['MAXNET'](x.squeeze().float()))\n",
        "\n",
        "        # Self\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qxXQ2eBvKry"
      },
      "outputs": [],
      "source": [
        "# Build OCON MaxNetwork Architecture\n",
        "torch.manual_seed(SEED)\n",
        "ocon_maxnet = OCON_MaxNet(n_units=12, act_fun='ReLU', eps=0.25)\n",
        "\n",
        "# MaxNet & Sub-Networks Parameters\n",
        "print('OCON MaxNet STATE')\n",
        "model_desc(ocon_maxnet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FC4B4Dajq_Ih"
      },
      "outputs": [],
      "source": [
        "def maxnet_algo(maxnet_function, n_units, act_fun, eps, input_array):\n",
        "    \"\"\"\n",
        "    MaxNet Re-iteration algorithm for Maximum Value retrieving from an input array\n",
        "    \"\"\"\n",
        "    non_zero_outs = np.count_nonzero(input_array)  # Non Zero Values initialization\n",
        "    maxnet_in = torch.from_numpy(input_array)  # MaxNet Input Tensor initialization\n",
        "\n",
        "    results = []  # Results initialization\n",
        "\n",
        "    counter = 0\n",
        "    while non_zero_outs != 1:\n",
        "        counter += 1\n",
        "\n",
        "        # Create the MaxNet\n",
        "        torch.manual_seed(SEED)  # Redundant\n",
        "        maxnet = maxnet_function(n_units = n_units, act_fun = act_fun, eps = eps)\n",
        "\n",
        "        # Compute Forward Pass\n",
        "        results = maxnet(maxnet_in)\n",
        "\n",
        "        # Non_zero outputs & Maxnet Input Update\n",
        "        non_zero_outs = np.count_nonzero(results.detach().numpy())\n",
        "        maxnet_in = results.detach()  # Save results for next iteration\n",
        "\n",
        "    print(f'Maximum Value found in {counter} iterations')\n",
        "    return np.argmax(results.detach().numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2Bx_2o3xYqA"
      },
      "outputs": [],
      "source": [
        "# MaxNet on Sub-Networks predictions\n",
        "ocon_predictions_prob = np.zeros((len(ocon_predictions), x_data_minmax.shape[0]))  # NumPy predictions matrix (12 * 1617)\n",
        "\n",
        "# Convert from List of Tensors to 2D NumPy Array\n",
        "for i in range(len(ocon_predictions)):\n",
        "    ocon_predictions_prob[i, :] = ocon_predictions[i].detach().squeeze().numpy()\n",
        "\n",
        "maxnet_class_predictions = []  # Classes Outputs list initialization\n",
        "# MaxNet application\n",
        "for i in range(x_data_minmax.shape[0]):\n",
        "    print(f'Dataset Sample({i + 1}) Class Evaluation')\n",
        "\n",
        "    samp_predictions = ocon_predictions_prob[:, i]  # Array of 12 predictions for each Dataset sample (OCON outputs)\n",
        "    class_prediction = maxnet_algo(OCON_MaxNet, n_units=12, act_fun='ReLU', eps=-0.1, input_array=samp_predictions)  # MaxNet Computation\n",
        "    maxnet_class_predictions.append(class_prediction)  # Result appending\n",
        "    print('-----------------------------------------------')\n",
        "\n",
        "maxnet_accuracy = 100 * np.mean((np.array(maxnet_class_predictions).reshape(1617, 1) == y_labels_raw_np))  # Accuracy computation\n",
        "print(f'Maxnet Output --> Phoneme ACCURACY: {maxnet_accuracy}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_j3NrwCATFm"
      },
      "outputs": [],
      "source": [
        "# Argmax on Sub-Networks predictions (...for multiple 1s probabilities MaxNet infinite loops)\n",
        "#ocon_predictions_prob = np.zeros((len(new_ocon_predictions), x_data_minmax.shape[0]))  # NumPy predictions matrix (12 * 1617)\n",
        "#\n",
        "# Convert from List of Tensors to 2D NumPy Array\n",
        "#for i in range(len(new_ocon_predictions)):\n",
        "#    ocon_predictions_prob[i, :] = new_ocon_predictions[i].detach().squeeze().numpy()\n",
        "#\n",
        "#maxnet_class_predictions = np.argmax(ocon_predictions_prob, axis=0)\n",
        "#maxnet_accuracy = 100 * np.mean((np.array(maxnet_class_predictions).reshape(1617, 1) == y_labels_raw_np))  # Accuracy computation\n",
        "#print(f'Maxnet Output ACCURACY: {maxnet_accuracy}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lskWof8qBoQV"
      },
      "outputs": [],
      "source": [
        "# Evaluation Analysis Plot\n",
        "plt.figure(figsize=(30, 5))\n",
        "plt.suptitle(f'OCON Bank + MaxNet Evaluation: {maxnet_accuracy:.0f}%')\n",
        "\n",
        "plot_x_ticks = end_idx[:]\n",
        "plot_x_ticks = np.delete(plot_x_ticks, len(end_idx) - 1)\n",
        "plot_y_ticks = [n for n in range(len(vowels))]\n",
        "\n",
        "plt.plot(y_labels_raw_np, 'rs', label='Ground Truths')\n",
        "plt.plot(maxnet_class_predictions, 'k.', label='MAXNET Outputs')\n",
        "plt.xlabel('Dataset samples')\n",
        "plt.xticks(ticks=plot_x_ticks, labels=vowels)\n",
        "plt.xlim([-10, len(y_labels_raw_np) + 10])\n",
        "plt.ylabel('Labels')\n",
        "plt.yticks(ticks=plot_y_ticks, labels=vowels)\n",
        "plt.legend(loc='best')\n",
        "plt.grid()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('OCON_model_evaluation')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QS6s2D7TKMIE"
      },
      "source": [
        "## *Further improvements*\n",
        "\n",
        "> 1) Repeat Training Cycle in order to increase Loss minimization $(<0.1, <0.05)$\n",
        ">\n",
        "> 2) Other Datasets Evaluation\n",
        ">\n",
        "> 3) Ratios multi-resolution transform (Bark, ERB, Mel)\n",
        ">\n",
        "> 4) Dataset Augmentation via Pitch Shifting: mean fund, mean ratios + artifical randomness (variance)\n",
        ">\n",
        "\n",
        "---\n",
        "\n",
        "> 1) Spectral features (HGCW dataset)\n",
        ">"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}