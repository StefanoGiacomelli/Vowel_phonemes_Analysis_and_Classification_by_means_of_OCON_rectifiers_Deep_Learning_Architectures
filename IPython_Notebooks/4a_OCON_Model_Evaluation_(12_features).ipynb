{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ***OCON Model Evaluation***\n",
        "*(12-features Complete Dataset)*\n",
        "\n",
        "**Author:** S. Giacomelli\n",
        "\n",
        "**Year:** 2023\n",
        "\n",
        "**Affiliation:** UniversitÃ  degli studi di L'Aquila (Ph.D. candidate)\n",
        "\n",
        "**Master Degree Thesis**: \"*Vowel phonemes Analysis & Classification by means of OCON rectifiers Deep Learning Architectures*\"\n",
        "\n",
        "**Description:** One-Class-One-Network (OCON) Model metrics & evaluation"
      ],
      "metadata": {
        "id": "WlhhrR6iozg2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import scripts"
      ],
      "metadata": {
        "id": "Rhbo63F6Z-wY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3HyALgsnqGV"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"mnt\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Change Directory to Notebooks folder\n",
        "%cd \"mnt/MyDrive/Colab Notebooks\"\n",
        "%ls"
      ],
      "metadata": {
        "id": "9_cwSwtZMKlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import library install\n",
        "!pip install import-ipynb\n",
        "import import_ipynb"
      ],
      "metadata": {
        "id": "Q2VR3JjVMOpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import superdlpy as DL  # Supervised-DL library"
      ],
      "metadata": {
        "id": "t5jIcJM4MRDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib_inline\n",
        "matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n",
        "\n",
        "SEED = 42"
      ],
      "metadata": {
        "id": "YphvFPulXM23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Return to \"content\" folder (COLAB default)\n",
        "%cd /content"
      ],
      "metadata": {
        "id": "-xoV3wkDwkDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "55wYLTLzLvez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HGCW_dataset_utils = np.load(file='./HGCW_dataset_utils.npz')\n",
        "print('Raw features                    Data shape:', HGCW_dataset_utils['HGCW_raw'].shape)\n",
        "print('Fundamental Normalized features Data shape:', HGCW_dataset_utils['HGCW_fund_norm'].shape)\n",
        "print('MinMax features                 Data shape:', HGCW_dataset_utils['HGCW_minmax'].shape)\n",
        "print('Labels                          Data shape:', HGCW_dataset_utils['HGCW_labels'].shape)\n",
        "print('Classes size                    Data shape:', HGCW_dataset_utils['classes_size'].shape)\n",
        "print('Classes indices                 Data shape:', HGCW_dataset_utils['classes_idx'].shape)\n",
        "\n",
        "x_data_raw_np = HGCW_dataset_utils['HGCW_raw']\n",
        "x_data_fund_norm = HGCW_dataset_utils['HGCW_fund_norm']\n",
        "x_data_minmax = HGCW_dataset_utils['HGCW_minmax']\n",
        "y_labels_raw_np = HGCW_dataset_utils['HGCW_labels']\n",
        "vow_size = HGCW_dataset_utils['classes_size']\n",
        "end_idx = HGCW_dataset_utils['classes_idx']\n",
        "\n",
        "# Auxiliary lists\n",
        "vowels = ['ae', 'ah', 'aw', 'eh', 'er', 'ei', 'ih', 'iy', 'oa', 'oo', 'uh', 'uw']  # Vowels list\n",
        "colors = ['red', 'saddlebrown', 'darkorange', 'darkgoldenrod', 'gold', 'darkkhaki', 'olive', 'darkgreen', 'steelblue', 'fuchsia', 'indigo', 'black']"
      ],
      "metadata": {
        "id": "NHdRpd_NpAO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Architectures & Model (initialization)\n",
        "(see \"*One-Class_Sub-Network_Analysis.ipynb*\")\n",
        "\n",
        "```\n",
        "Multi-Layer Perceptron (Binary Logistic Regression)\n",
        "- Input Layer: 3x4 features (formant ratios, min-max normalized)\n",
        "- Hidden Layer: 100 units\n",
        "- Output Layer: 1 normalized probability\n",
        "---------------------------------------------------------------\n",
        "- Learning Rate: 0.0001 (10^-4)\n",
        "- Optimizer: Adam (Adaptive Momentum)\n",
        "---------------------------------------------------------------\n",
        "- Mini-Batch Training:\n",
        "    . Re-iterated Sub-Dataset Shuffling\n",
        "    . Early Stopping (Test Accuracy driven)\n",
        "    . Batch size = 32\n",
        "---------------------------------------------------------------\n",
        "- Regularization:\n",
        "    . Weight Decay (L2 Penalty): 0.0001 (10^-4)\n",
        "    . DropOut:\n",
        "        * Input Layer Drop Rate: 0.8\n",
        "        * Hidden Layer Drop Rate: 0.5.\n",
        "    . Batch Normalization\n",
        "```\n",
        "```\n",
        "- Ensemble Training:\n",
        "    . Epochs: 1000  (for each \"Data Batch-Set\")\n",
        "    . Early Stop Loss breakpoint: 0.15\n",
        "    . Early Stop Accuracy breakpoint: 95%\n",
        "    . Shuffling classes tolerance: 0.01\n",
        "```"
      ],
      "metadata": {
        "id": "5Z00-2H5Z2IP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# OCON Model init: 12 MLP binary classifiers (input = 12(3x4) flattened features vector)\n",
        "classifiers_bank = DL.OCON_model(DL.MLP_bin_classifier, class_labels=vowels, seed=SEED, in_features=12, hidden_units=100, act_fun='ReLU', rate_in=0.8, rate_hidden=0.5)"
      ],
      "metadata": {
        "id": "jNZeKt6Te_kT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Pre-trained states (for each MLP classifier)\n",
        "states_path = [\"ae_subnet_Params.pth\",\n",
        "               \"ah_subnet_Params.pth\",\n",
        "               \"aw_subnet_Params.pth\",\n",
        "               \"eh_subnet_Params.pth\",\n",
        "               \"er_subnet_Params.pth\",\n",
        "               \"ei_subnet_Params.pth\",\n",
        "               \"ih_subnet_Params.pth\",\n",
        "               \"iy_subnet_Params.pth\",\n",
        "               \"oa_subnet_Params.pth\",\n",
        "               \"oo_subnet_Params.pth\",\n",
        "               \"uh_subnet_Params.pth\",\n",
        "               \"uw_subnet_Params.pth\"]\n",
        "\n",
        "for i in range(len(classifiers_bank)):\n",
        "    DL.model_state_io(classifiers_bank[i], states_path[i], mode='load')"
      ],
      "metadata": {
        "id": "DDRkpIOmxPxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Description\n",
        "for i in range(len(classifiers_bank)):\n",
        "    print(f'OCON \"{classifiers_bank[i].name}\" Classifier STATE')\n",
        "    DL.model_state(classifiers_bank[i], input_size=(1, 12))\n",
        "    print()"
      ],
      "metadata": {
        "id": "Q0RXPHGAf6kI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test Correctness\n",
        "ocon_predictions, ocon_dist_errors, ocon_eval_accuracies, ocon_g_truths = DL.OCON_eval(classifiers_bank, features_data=x_data_minmax[:, 1:], labels=y_labels_raw_np)"
      ],
      "metadata": {
        "id": "S4hthTgDpIfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation Metrics"
      ],
      "metadata": {
        "id": "HBDAanlP2CRI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Post-Training evaluation\n",
        "- Accuracy\n",
        "- *Predicted --> Measured* error\n",
        "- Positive predictions PMD"
      ],
      "metadata": {
        "id": "WvTWkyNC3ylE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Evaluation Analysis Plot\n",
        "plt.figure(figsize=(12, 3 * len(classifiers_bank)))\n",
        "plot_ticks = end_idx[:]\n",
        "plot_ticks = np.delete(plot_ticks, -1)\n",
        "\n",
        "for i in range(len(classifiers_bank)):\n",
        "    plt.subplot(len(classifiers_bank), 3, (i * 3) + 1)\n",
        "    plt.plot(ocon_predictions[i], 'k.', label='Raw Predictions')\n",
        "    plt.plot(ocon_g_truths[i], 'rx', label='Ground Truths')\n",
        "    plt.axhline(0.5, linestyle='--', color='grey')\n",
        "    plt.title(f'{classifiers_bank[i].name.upper()} Predictions Accuracy: {ocon_eval_accuracies[i]:.2f}%')\n",
        "    plt.xlabel('Data (Indices)')\n",
        "    plt.xticks(ticks=plot_ticks, labels=vowels)\n",
        "    plt.ylabel('Normalized Probability')\n",
        "    plt.grid()\n",
        "    plt.legend(loc='best')\n",
        "\n",
        "    plt.subplot(len(classifiers_bank), 3, (i * 3) + 2)\n",
        "    plt.plot(ocon_dist_errors[i], 'k')\n",
        "    plt.title(f'Predicted to Measured Error')\n",
        "    plt.xlabel('Data (Indices)')\n",
        "    plt.xticks(ticks=plot_ticks, labels=vowels)\n",
        "    plt.ylabel('Normalized Probability Error')\n",
        "    plt.ylim([-1.1, 1.1])\n",
        "    plt.grid()\n",
        "\n",
        "    plt.subplot(len(classifiers_bank), 3, (i * 3) + 3)\n",
        "\n",
        "    # Predictions list processing\n",
        "    predictions_temp = ocon_predictions[i]\n",
        "    class_predictions = [item for sublist in predictions_temp for item in sublist]  # Turn a list of lists in a single list (of tensors)\n",
        "    for j in range(len(class_predictions)):  # Turn a list of tensors of one variable in a list of scalars (item() method)\n",
        "        class_predictions[j] = class_predictions[j].item()\n",
        "\n",
        "    # Positives & False-Positives extraction\n",
        "    positives = []\n",
        "    for w in range(len(vowels)):\n",
        "        num = (np.array(class_predictions[end_idx[w]: end_idx[w + 1]]) > 0.5).sum()\n",
        "        positives.append(num)\n",
        "\n",
        "    plt.bar(np.arange(len(vowels)), positives, color='k')\n",
        "    plt.title(f'\"{vowels[i]}\" Positive Probabilities Distribution')\n",
        "    plt.xlabel('Normalized Probabilities')\n",
        "    plt.ylabel('Occurences (in samples)')\n",
        "    plt.xticks([n for n in range(12)], vowels)\n",
        "    plt.grid()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rDdkvTTT08Yt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### One-Class Evaluation (Balanced Sub-sets)\n",
        "- Confusion Matrix & features\n",
        "- ROC-AUC & features"
      ],
      "metadata": {
        "id": "KDnub9fejw9C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_list = []\n",
        "matrices = []\n",
        "sub_datasets_size = []\n",
        "for i in range(len(classifiers_bank)):\n",
        "    # One-Hot Encoding\n",
        "    sub_data, sub_data_labels_bin, sub_data_labels = DL.one_hot_encoder(sel_class_number=i, x_data=x_data_minmax, labels_tot=12, classes_size=vow_size, classes_idx=end_idx, debug=False)\n",
        "    sub_datasets_size.append(sub_data.shape[0])\n",
        "\n",
        "    # Compute i-esimal One-Class predictions\n",
        "    predictions_proba = classifiers_bank[i](torch.tensor(sub_data[:, 1:]).float())\n",
        "\n",
        "    # Compute i-esimal Confusion Matrix\n",
        "    conf_ordered, metrics = DL.binary_eval((predictions_proba.detach() > 0.5).float().numpy().T, sub_data_labels_bin, plot=False)\n",
        "    metrics_list.append(metrics)\n",
        "    matrices.append(conf_ordered)\n",
        "\n",
        "# Plot Results\n",
        "plt.figure(figsize=(12, 3.5 * len(classifiers_bank)))\n",
        "\n",
        "for i in range(len(classifiers_bank)):\n",
        "    plt.subplot(len(classifiers_bank), 2, (i * 2) + 1)\n",
        "    plt.bar(np.arange(4), metrics_list[i], 0.4, color='k',\n",
        "            label=f'Accuracy: {metrics_list[i][0]:.2f}%\\nPrecision: {metrics_list[i][1]:.2f}%\\nRecall: {metrics_list[i][2]:.2f}%\\nF1-Score: {metrics_list[i][3]:.2f}%')\n",
        "    plt.title(f'{classifiers_bank[i].name.upper()} Metrics')\n",
        "    plt.xlabel('Metrics')\n",
        "    plt.ylabel('Values (in %)')\n",
        "    plt.xticks([0, 1, 2, 3], ['Accuracy', 'Precision', 'Recall', 'F1 Score'])\n",
        "    plt.ylim([80, 100])\n",
        "    plt.grid()\n",
        "    plt.legend(loc='best')\n",
        "\n",
        "    plt.subplot(len(classifiers_bank), 2, (i * 2) + 2)\n",
        "    plt.imshow(matrices[i], cmap='RdBu', vmin = 0, vmax = sub_datasets_size[i] // 2)\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.xlabel('Objective Values')\n",
        "    plt.ylabel('Predicted Values')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.xticks([0, 1], ['True','False'])\n",
        "    plt.yticks([0, 1], ['True','False'])\n",
        "    plt.colorbar(label='Rate (in samples)')\n",
        "\n",
        "    plt.text(0, 0,f'True Positives:\\n{matrices[i][0,0]}' ,ha='center',va='center', color='k')\n",
        "    plt.text(0, 1,f'False Negatives:\\n{matrices[i][1,0]}',ha='center',va='center', color='k')\n",
        "    plt.text(1, 1,f'True Negatives:\\n{matrices[i][1,1]}' ,ha='center',va='center', color='k')\n",
        "    plt.text(1, 0,f'False Positives:\\n{matrices[i][0,1]}',ha='center',va='center', color='k')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "eir510lw-bqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ROC-AUC Score & Features\n",
        "roc_metrics_list = []\n",
        "roc_plot_measures = []\n",
        "for i in range(len(classifiers_bank)):\n",
        "    # One-Hot Encoding\n",
        "    sub_data, sub_data_labels_bin, sub_data_labels = DL.one_hot_encoder(sel_class_number=i, x_data=x_data_minmax, labels_tot=12, classes_size=vow_size, classes_idx=end_idx, debug=False)\n",
        "    sub_datasets_size.append(sub_data.shape[0])\n",
        "\n",
        "    # Compute i-esimal One-Class predictions\n",
        "    predictions_proba = classifiers_bank[i](torch.tensor(sub_data[:, 1:]).float())\n",
        "\n",
        "    model_predictions = (predictions_proba.detach() > 0.5).float().numpy()\n",
        "    ground_truths = sub_data_labels_bin\n",
        "\n",
        "    # Features computation\n",
        "    true_positives = len(np.where((model_predictions == 1) & (ground_truths == 1))[0])\n",
        "    false_positives = len(np.where((model_predictions == 1) & (ground_truths == 0))[0])\n",
        "    true_negatives = len(np.where((model_predictions == 0) & (ground_truths == 0))[0])\n",
        "    false_negatives = len(np.where((model_predictions == 0) & (ground_truths == 1))[0])\n",
        "    true_positive_rate = (true_positives / (true_positives + false_negatives)) * 100\n",
        "    true_negative_rate = (true_negatives / (true_negatives + false_positives)) * 100\n",
        "    false_positive_rate = 100 - true_negative_rate\n",
        "    false_negative_rate = 100 - true_positive_rate\n",
        "\n",
        "    print(f'--------{classifiers_bank[i].name.upper()} ROC-AUC & DET Metrics--------')\n",
        "    print(f'True Positive Rate      (TPR)   : {true_positive_rate:.2f}%')\n",
        "    print(f'True Negative Rate      (TNR)   : {true_negative_rate:.2f}%')\n",
        "    print(f'False Positive Rate     (FPR)   : {false_positive_rate:.2f}%')\n",
        "    print(f'False Negative Rate     (FNR)   : {false_negative_rate:.2f}%')\n",
        "    print('------------------------------')\n",
        "    print(f'Error Rate                      : {((false_positives + false_negatives) / ground_truths.size):.2f}')\n",
        "    print(f'False Discovery Rate    (FDR)   : {(false_positives / (false_positives + true_positives)):.2f}')\n",
        "    print(f'False Omission Rate     (FOR)   : {(false_negatives / (false_negatives + true_negatives)):.2f}')\n",
        "    print(f'Negative Predicted Values Index : {(true_negatives / (true_negatives + false_negatives)):.2f}')\n",
        "    print('-----------------------------------------------')\n",
        "    print()\n",
        "\n",
        "    _, _, _, _, auc, fpr, tpr = DL.binary_roc_auc_det(predictions_proba.detach().float().numpy(), ground_truths, plot=False)\n",
        "\n",
        "    roc_metrics_list.append([true_positive_rate, false_positive_rate, true_negative_rate, false_negative_rate, auc])\n",
        "    roc_plot_measures.append([fpr, tpr, 1-tpr])\n",
        "\n",
        "# Plot Results\n",
        "plt.figure(figsize=(12, 4 * len(classifiers_bank)))\n",
        "for i in range(len(classifiers_bank)):\n",
        "    plt.subplot(len(classifiers_bank), 2, (i * 2) + 1)\n",
        "    plt.title(f'{classifiers_bank[i].name.upper()} ROC - AUC Score: {roc_metrics_list[i][4]:.2f}%')\n",
        "    plt.plot([0, 1], [0, 1], linestyle='--', color='blue', label='Random classification')\n",
        "    plt.plot(roc_plot_measures[i][0], roc_plot_measures[i][1], color='k')\n",
        "    plt.fill_between(roc_plot_measures[i][0], roc_plot_measures[i][1], color='green', alpha=0.4, label='RoI')\n",
        "    plt.fill_between((0.5, 1), (0, 0.5), step='pre', color='red', label='Low Sensitivity/Specificity')\n",
        "    plt.xlabel('False Positive Rate - (1 - Specificity) (norm. %)')\n",
        "    plt.ylabel('True Positive Rate - Sensitivity / Recall (norm. %)')\n",
        "    plt.grid()\n",
        "    plt.xticks(np.arange(0, 1.1, 0.1))\n",
        "    plt.yticks(np.arange(0, 1.1, 0.1))\n",
        "    plt.legend(loc='best')\n",
        "\n",
        "    plt.subplot(len(classifiers_bank), 2, (i * 2) + 2)\n",
        "    plt.title(f'Detection Errors Tradeoff curve')\n",
        "    plt.plot(roc_plot_measures[i][0], roc_plot_measures[i][2], 'k')\n",
        "    plt.xscale('log')\n",
        "    plt.yscale('log')\n",
        "    plt.xlabel('Log(False Positive Rate)')\n",
        "    plt.ylabel('Log(False Negative Rate)')\n",
        "    plt.grid()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RPuO5kqpB8ZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### One-Class evaluation (Overall Dataset)\n",
        "- Confusion Matrix & features\n",
        "- ROC-AUC & features"
      ],
      "metadata": {
        "id": "Py6dSEAGYOib"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion Matrix & Features\n",
        "metrics_list = []\n",
        "matrices = []\n",
        "for i in range(len(classifiers_bank)):\n",
        "    conf_ordered, metrics = DL.binary_eval((ocon_predictions[i] > 0.5).float().numpy().T, ocon_g_truths[i], plot=False)\n",
        "    metrics_list.append(metrics)\n",
        "    matrices.append(conf_ordered)\n",
        "\n",
        "# Plot Results\n",
        "plt.figure(figsize=(12, 3.5 * len(classifiers_bank)))\n",
        "\n",
        "for i in range(len(classifiers_bank)):\n",
        "    plt.subplot(len(classifiers_bank), 2, (i * 2) + 1)\n",
        "    plt.bar(np.arange(4), metrics_list[i], 0.4, color='k',\n",
        "            label=f'Accuracy: {metrics_list[i][0]:.2f}%\\nPrecision: {metrics_list[i][1]:.2f}%\\nRecall: {metrics_list[i][2]:.2f}%\\nF1-Score: {metrics_list[i][3]:.2f}%')\n",
        "    plt.axhline(y=50, color='grey', linestyle='--')\n",
        "    plt.title(f'{classifiers_bank[i].name.upper()} Metrics')\n",
        "    plt.xlabel('Metrics')\n",
        "    plt.ylabel('Values (in %)')\n",
        "    plt.xticks([0, 1, 2, 3], ['Accuracy', 'Precision', 'Recall', 'F1 Score'])\n",
        "    plt.ylim([0, 100])\n",
        "    plt.grid()\n",
        "    plt.legend(loc='best')\n",
        "\n",
        "    plt.subplot(len(classifiers_bank), 2, (i * 2) + 2)\n",
        "    plt.imshow(matrices[i], cmap='RdBu', vmin = 0, vmax = ocon_g_truths[i].shape[0] // 2)\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.xlabel('Objective Values')\n",
        "    plt.ylabel('Predicted Values')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.xticks([0, 1], ['True','False'])\n",
        "    plt.yticks([0, 1], ['True','False'])\n",
        "    plt.colorbar(label='Rate (in samples)')\n",
        "\n",
        "    plt.text(0, 0,f'True Positives:\\n{matrices[i][0,0]}' ,ha='center',va='center', color='k')\n",
        "    plt.text(0, 1,f'False Negatives:\\n{matrices[i][1,0]}',ha='center',va='center', color='k')\n",
        "    plt.text(1, 1,f'True Negatives:\\n{matrices[i][1,1]}' ,ha='center',va='center', color='k')\n",
        "    plt.text(1, 0,f'False Positives:\\n{matrices[i][0,1]}',ha='center',va='center', color='k')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YHPPPjcpdyiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ROC-AUC Score & Features\n",
        "roc_metrics_list = []\n",
        "roc_plot_measures = []\n",
        "for i in range(len(classifiers_bank)):\n",
        "    model_predictions = (ocon_predictions[i] > 0.5).float().numpy()\n",
        "    ground_truths = ocon_g_truths[i]\n",
        "\n",
        "    # Features computation\n",
        "    true_positives = len(np.where((model_predictions == 1) & (ground_truths == 1))[0])\n",
        "    false_positives = len(np.where((model_predictions == 1) & (ground_truths == 0))[0])\n",
        "    true_negatives = len(np.where((model_predictions == 0) & (ground_truths == 0))[0])\n",
        "    false_negatives = len(np.where((model_predictions == 0) & (ground_truths == 1))[0])\n",
        "    true_positive_rate = (true_positives / (true_positives + false_negatives)) * 100\n",
        "    true_negative_rate = (true_negatives / (true_negatives + false_positives)) * 100\n",
        "    false_positive_rate = 100 - true_negative_rate\n",
        "    false_negative_rate = 100 - true_positive_rate\n",
        "\n",
        "    print(f'--------{classifiers_bank[i].name.upper()} ROC-AUC & DET Metrics--------')\n",
        "    print(f'True Positive Rate      (TPR)   : {true_positive_rate:.2f}%')\n",
        "    print(f'True Negative Rate      (TNR)   : {true_negative_rate:.2f}%')\n",
        "    print(f'False Positive Rate     (FPR)   : {false_positive_rate:.2f}%')\n",
        "    print(f'False Negative Rate     (FNR)   : {false_negative_rate:.2f}%')\n",
        "    print('------------------------------')\n",
        "    print(f'Error Rate                      : {((false_positives + false_negatives) / ground_truths.size):.2f}')\n",
        "    print(f'False Discovery Rate    (FDR)   : {(false_positives / (false_positives + true_positives)):.2f}')\n",
        "    print(f'False Omission Rate     (FOR)   : {(false_negatives / (false_negatives + true_negatives)):.2f}')\n",
        "    print(f'Negative Predicted Values Index : {(true_negatives / (true_negatives + false_negatives)):.2f}')\n",
        "    print('-----------------------------------------------')\n",
        "    print()\n",
        "\n",
        "    true_positive_rate, false_positive_rate, true_negative_rate, false_negative_rate, auc, fpr, tpr = DL.binary_roc_auc_det(ocon_predictions[i].float().numpy(), ground_truths, plot=False)\n",
        "\n",
        "    roc_metrics_list.append([true_positive_rate, false_positive_rate, true_negative_rate, false_negative_rate, auc])\n",
        "    roc_plot_measures.append([fpr, tpr, 1-tpr])\n",
        "\n",
        "# Plot Results\n",
        "plt.figure(figsize=(12, 4 * len(classifiers_bank)))\n",
        "for i in range(len(classifiers_bank)):\n",
        "    plt.subplot(len(classifiers_bank), 2, (i * 2) + 1)\n",
        "    plt.title(f'{classifiers_bank[i].name.upper()} ROC - AUC Score: {roc_metrics_list[i][4]:.2f}%')\n",
        "    plt.plot([0, 1], [0, 1], linestyle='--', color='blue', label='Random classification')\n",
        "    plt.plot(roc_plot_measures[i][0], roc_plot_measures[i][1], color='k')\n",
        "    plt.fill_between(roc_plot_measures[i][0], roc_plot_measures[i][1], color='green', alpha=0.4, label='RoI')\n",
        "    plt.fill_between((0.5, 1), (0, 0.5), step='pre', color='red', label='Low Sensitivity/Specificity')\n",
        "    plt.xlabel('False Positive Rate - (1 - Specificity) (norm. %)')\n",
        "    plt.ylabel('True Positive Rate - Sensitivity / Recall (norm. %)')\n",
        "    plt.grid()\n",
        "    plt.xticks(np.arange(0, 1.1, 0.1))\n",
        "    plt.yticks(np.arange(0, 1.1, 0.1))\n",
        "    plt.legend(loc='best')\n",
        "\n",
        "    plt.subplot(len(classifiers_bank), 2, (i * 2) + 2)\n",
        "    plt.title(f'Detection Errors Tradeoff curve')\n",
        "    plt.plot(roc_plot_measures[i][0], roc_plot_measures[i][2], 'k')\n",
        "    plt.xscale('log')\n",
        "    plt.yscale('log')\n",
        "    plt.xlabel('Log(False Positive Rate)')\n",
        "    plt.ylabel('Log(False Negative Rate)')\n",
        "    plt.grid()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2QH27lzusps9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MaxNetwork (*ensembling*)"
      ],
      "metadata": {
        "id": "miUwwDqt4ExQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MaxNet predictions matrix\n",
        "ocon_predictions_prob = np.zeros((len(ocon_predictions), x_data_minmax.shape[0]))  # NumPy predictions matrix (12 * 1617)\n",
        "\n",
        "# Convert from List of Tensors to 2D NumPy Array\n",
        "for i in range(len(ocon_predictions)):\n",
        "    ocon_predictions_prob[i, :] = ocon_predictions[i].detach().squeeze().numpy()\n",
        "\n",
        "maxnet_class_predictions = []  # Classes Outputs list initialization\n",
        "# MaxNet application\n",
        "for i in range(x_data_minmax.shape[0]):\n",
        "    print(f'Dataset Sample({i + 1}) Class Evaluation')\n",
        "\n",
        "    samp_predictions = ocon_predictions_prob[:, i]  # Array of 12 predictions for each Dataset sample (OCON outputs)\n",
        "    class_prediction = DL.MaxNet_algo(samp_predictions, DL.MaxNet, n_units=12, eps=-0.1)  # MaxNet Instance\n",
        "    maxnet_class_predictions.append(class_prediction)  # Result appending\n",
        "    print('-----------------------------------------------')\n",
        "\n",
        "maxnet_accuracy = 100 * np.mean((np.array(maxnet_class_predictions).reshape(1597, 1) == y_labels_raw_np))  # Accuracy computation\n",
        "print(f'Maxnet Output --> Phoneme ACCURACY: {maxnet_accuracy}%')"
      ],
      "metadata": {
        "id": "AcCHf2ozvYeH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MaxNet Evaluation Plot\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.suptitle(f'OCON Bank --> MaxNet: {maxnet_accuracy:.0f}% Accuracy')\n",
        "\n",
        "plot_x_ticks = end_idx[:]\n",
        "plot_x_ticks = np.delete(plot_x_ticks, len(end_idx) - 1)\n",
        "plot_y_ticks = [n for n in range(len(vowels))]\n",
        "\n",
        "plt.plot(y_labels_raw_np, 'rs', label='Ground Truths')\n",
        "plt.plot(maxnet_class_predictions, 'k.', label='MAXNET Outputs')\n",
        "plt.xlabel('Dataset Labels')\n",
        "plt.xticks(ticks=plot_x_ticks, labels=vowels)\n",
        "plt.xlim([-10, len(y_labels_raw_np) + 10])\n",
        "plt.ylabel('Predicted Labels')\n",
        "plt.yticks(ticks=plot_y_ticks, labels=vowels)\n",
        "plt.legend(loc='best')\n",
        "plt.grid()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kRzOF7T2XXTj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}